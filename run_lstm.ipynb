{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparamters - ones that do not change through training\n",
    "\n",
    "batchsize = 100       # number of samples to work through before updating the model's internal parameters\n",
    "sequence_len = 28     # how much information the model can retain from the past and how long it can backpropagate gradients through time\n",
    "input_len = 28        # sequential data in the form of fixed-length input sequences\n",
    "hidden_size = 128     # number of features that are used to compute each hidden state\n",
    "num_layers = 2        # stacking two LSTMs together to form a stacked LSTM\n",
    "num_classes = 10      # number of distinct classes in the classification task. this determines the size of the output layer of the network\n",
    "num_epochs = 5        # number of times the entire training dataset will pass through the model\n",
    "learning_rate = 0.01  # step size used for updating the model parameters during training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the training and testing data\n",
    "#\n",
    "#   datasets.FashionMNIST function from the torchvision.datasets module that is used to download and load the FashionMNIST dataset\n",
    "#   root: specifies the directory where the dataset is stored \n",
    "#   train: whether or not the dataset will that is being loaded is the training set\n",
    "#   transform: applies a transformation to the data, converting them to the specified form (PyTorch tensors in this case)\n",
    "#\n",
    "training_data = datasets.FashionMNIST(root=\"/Users/jaimejacob/Documents/ml_practice/data\", train=True, transform=transforms.ToTensor())\n",
    "test_data = datasets.FashionMNIST(root=\"/Users/jaimejacob/Documents/ml_practice/data\", train=False, transform=transforms.ToTensor())\n",
    "\n",
    "\n",
    "# Creating the Training and Testing DataLoader\n",
    "#\n",
    "#   DataLoader: a PyTorch utility that wraps the dataset and provides an iterable over the dataset\n",
    "#   first parameter (either training_data or test_data): the dataset to be wrapped by the DataLoader\n",
    "#   batch_size: number of samples that will be loaded per batch\n",
    "#\n",
    "train_dataloader = DataLoader(training_data, batch_size=batchsize)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LSTM class\n",
    "class LSTM(nn.Module):  # defines a new class 'LSTM' that inherits from 'torch.nn.module' which is the base class for all neural networks and models\n",
    "\n",
    "    # Initializer method defining the layers and parameters of the network\n",
    "    def __init__(self, input_len, hidden_size, num_class, num_layers):\n",
    "        super(LSTM, self).__init__()    # calls the initializer of nn.module to ensure the module is set up correctly \n",
    "        self.hidden_size = hidden_size  # stores the hidden_size parameter\n",
    "        self.num_layers = num_layers    # stores the num_layers parameter\n",
    "        self.lstm = nn.LSTM(input_len, hidden_size, num_layers, batch_first=True)  # defines an LSTM layer with the specified input length, hidden size, and num layers\n",
    "        self.output_layer = nn.Linear(hidden_size, num_class)  # defines a linear layer that maps the hidden state output of the LSTM to the desired number of classes\n",
    "\n",
    "    # Defines the forward pass of the network\n",
    "    #   X : input tensor\n",
    "    def forward(self, X):\n",
    "        hidden_states = torch.zeros(self.num_layers, X.size(0), self.hidden_size)  # initializes the hidden sates with zeros\n",
    "        cell_states  =  torch.zeros(self.num_layers, X.size(0), self.hidden_size)  # initializes the cell states with zeros\n",
    "       \n",
    "        # Pass the input tensor, X, and the intiial hidden and cell states through the LSTM layer\n",
    "        #   Out contains the output features from the LSTM for each time step\n",
    "        #   _ contains the hidden and cell states\n",
    "        out, _ = self.lstm(X, (hidden_states, cell_states))\n",
    "\n",
    "        # Takes the output from the last time step of the LSTM and passes it through the fully connected layer to get the final output\n",
    "        #   Size output is the number of classes, as defined in initialization\n",
    "        out = self.output_layer(out[:, -1, :])\n",
    "\n",
    "        return out  # returns the final output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (lstm): LSTM(28, 128, num_layers=2, batch_first=True)\n",
      "  (output_layer): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the LSTM model we defined above\n",
    "#   Using the hyperparameters we defined earlier as well\n",
    "model = LSTM(input_len, hidden_size, num_classes, num_layers)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining more hyperparameters\n",
    "\n",
    "# Loss Function - quantifies the difference between the predicted outputs of a machine learning algorithm and the actual target values\n",
    "#\n",
    "#   Cross Entropy is a commonly used loss function for multi-class classification tasks\n",
    "#   Loss function of choice can change depending on objective when building LSTM\n",
    "#\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer - function or an algorithm that adjusts the attributes of the neural network\n",
    "#\n",
    "#   Optimizers aid in reducing the overall loss (calculated by the loss function) and improving accuracy\n",
    "#   Adam is an optimization alogorithm well-suited for training deep neural networks \n",
    "#   Uses Stochastic Gradient Descent but allows for different step sizes for different parameters \n",
    "#   \n",
    "#   model.parameters(): Provides the parameters (weights and biases) of the model we instanstiated with our defined LSTM model that need to be optimized during training \n",
    "#   lr: specifies the learning rate, which controls the step size in the parameter space update\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training function\n",
    "#\n",
    "#   num_epochs: number of times to iterate through the entire dataset\n",
    "#   model: the neural network to be trained\n",
    "#   train_dataloader: PyTorch DataLoader object that provides batches of training data\n",
    "#   loss_func: Loss function used to compute the loss between predictions and actual outputs\n",
    "#\n",
    "def train(num_epochs, model, train_dataloader, loss_func):\n",
    "    total_steps = len(train_dataloader)\n",
    "\n",
    "    # Iterate over the specified number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # Iterate over batches of data provided by train_dataloader\n",
    "        for batch, (image, labels) in enumerate(train_dataloader):\n",
    "\n",
    "            image = image.reshape(-1, sequence_len, input_len)  # reshapes the input data to match the expected shape \n",
    "\n",
    "            output = model(image)  # passes the reshaped input through the model to get predictions\n",
    "            loss = loss_func(output, labels)  # computes the loss between the model's predictions and the actual labels\n",
    "\n",
    "            optimizer.zero_grad()  # clears the gradients of all optimized parameters, ensures the gradients are not accumualted and are reset to zero before backpropagation\n",
    "            loss.backward()   # computes gradients of the loss with respect to all model parameters\n",
    "            optimizer.step()  # updates the model parameters based on the computed gradients and the optimizer's update rule\n",
    "\n",
    "            # Print the current epoch, batch number, total number of batches, and the current loss every 100 batches\n",
    "            if (batch+ 1) % 100 == 0:\n",
    "                print(f\"Epoch:{epoch}; Batch {batch + 1} / {total_steps}; Loss: {loss.item():>4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0; Batch 99 / 600; Loss: 0.909919\n",
      "Epoch:0; Batch 199 / 600; Loss: 0.769844\n",
      "Epoch:0; Batch 299 / 600; Loss: 0.576003\n",
      "Epoch:0; Batch 399 / 600; Loss: 0.528650\n",
      "Epoch:0; Batch 499 / 600; Loss: 0.631615\n",
      "Epoch:0; Batch 599 / 600; Loss: 0.439263\n",
      "Epoch:1; Batch 99 / 600; Loss: 0.470505\n",
      "Epoch:1; Batch 199 / 600; Loss: 0.360890\n",
      "Epoch:1; Batch 299 / 600; Loss: 0.382278\n",
      "Epoch:1; Batch 399 / 600; Loss: 0.327946\n",
      "Epoch:1; Batch 499 / 600; Loss: 0.541091\n",
      "Epoch:1; Batch 599 / 600; Loss: 0.262329\n",
      "Epoch:2; Batch 99 / 600; Loss: 0.310740\n",
      "Epoch:2; Batch 199 / 600; Loss: 0.332401\n",
      "Epoch:2; Batch 299 / 600; Loss: 0.316494\n",
      "Epoch:2; Batch 399 / 600; Loss: 0.318100\n",
      "Epoch:2; Batch 499 / 600; Loss: 0.491146\n",
      "Epoch:2; Batch 599 / 600; Loss: 0.278895\n",
      "Epoch:3; Batch 99 / 600; Loss: 0.337027\n",
      "Epoch:3; Batch 199 / 600; Loss: 0.264335\n",
      "Epoch:3; Batch 299 / 600; Loss: 0.349030\n",
      "Epoch:3; Batch 399 / 600; Loss: 0.286830\n",
      "Epoch:3; Batch 499 / 600; Loss: 0.429619\n",
      "Epoch:3; Batch 599 / 600; Loss: 0.272788\n",
      "Epoch:4; Batch 99 / 600; Loss: 0.273469\n",
      "Epoch:4; Batch 199 / 600; Loss: 0.237056\n",
      "Epoch:4; Batch 299 / 600; Loss: 0.330258\n",
      "Epoch:4; Batch 399 / 600; Loss: 0.240411\n",
      "Epoch:4; Batch 499 / 600; Loss: 0.491039\n",
      "Epoch:4; Batch 599 / 600; Loss: 0.214066\n"
     ]
    }
   ],
   "source": [
    "# Train our model using the train function we just defined with previously defined parameters\n",
    "train(num_epochs, model, train_dataloader, loss_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9, 2, 1, 1, 6, 1, 4, 6, 5, 7, 4, 5, 7, 3, 4, 1, 2, 4, 8, 0, 2, 5, 7, 9,\n",
       "        1, 4, 6, 0, 9, 3, 8, 8, 3, 3, 8, 0, 7, 5, 7, 9, 6, 1, 3, 7, 6, 7, 2, 1,\n",
       "        2, 2, 4, 4, 5, 8, 2, 2, 8, 4, 8, 0, 7, 7, 8, 5, 1, 1, 2, 3, 9, 8, 7, 0,\n",
       "        2, 6, 2, 3, 1, 2, 8, 4, 1, 8, 5, 9, 5, 0, 3, 2, 0, 6, 5, 3, 6, 7, 1, 8,\n",
       "        0, 1, 4, 2])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define two new variables using the dataloader we defined earlier \n",
    "#   test_images will contain a batch of test images\n",
    "#   test_labels will contain the corresponding labels for those test images \n",
    "test_images, test_labels = next(iter(test_dataloader))\n",
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a forward pass of the test_images through your neural network model\n",
    "#   view reshapes test_images insto a 3D tensor with dimensions (batch_size, sequence_len, input_len)\n",
    "test_output = model(test_images.view(-1, 28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9, 2, 1, 1, 6, 1, 4, 6, 5, 7, 4, 5, 5, 3, 4, 1, 2, 2, 8, 0, 2, 5, 7, 5,\n",
       "        1, 2, 6, 0, 9, 3, 8, 8, 3, 3, 8, 0, 7, 5, 7, 9, 0, 1, 6, 7, 6, 7, 2, 1,\n",
       "        2, 6, 4, 4, 5, 8, 2, 2, 8, 4, 8, 0, 7, 7, 8, 5, 1, 1, 0, 3, 7, 8, 7, 0,\n",
       "        2, 6, 2, 3, 1, 2, 8, 4, 1, 8, 5, 9, 5, 0, 3, 2, 0, 6, 5, 3, 6, 7, 1, 8,\n",
       "        0, 1, 4, 2])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtain the predicted class labels from test_output\n",
    "#   torch.max is a PyTorch function that retrurns the maximum value of elements along a given dimension of a tensor \n",
    "_, predicted = torch.max(test_output, 1)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the length to compare, ensures that you are only comparing labels for which predictions are available in both sets\n",
    "length = min(len(predicted), len(test_labels))\n",
    "\n",
    "# Iterate over indices up to length \n",
    "#   Check if the predicted label matches the actual label\n",
    "#   If correct, append 1 to the correct list, otherwise 0\n",
    "correct = [1 for i in range(length) if predicted[i] == test_labels[i]]\n",
    "\n",
    "# Calculate percentage correct\n",
    "#   Sum the 1s in correct giving the total number of correct predictions\n",
    "#   Divide by the total number of entries to find the percentage correct \n",
    "percentage_correct = sum(correct)/length\n",
    "percentage_correct"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lstm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
